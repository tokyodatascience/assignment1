Dear All,

 

Here is a summary of the coding task I would like you to complete that we discussed in the 10 am class and that we will discuss this week in the 7 pm class.

 

(1) Please replace the linear function with a quadratic function alpha +_beta*x + gamma*x**2. Feel free to name your variables in any way you like. Suggested value of gamma_true is 8.0, but feel free to use any other value. Create the corresponding animation.

 

(2) Implement the following optimizers:

 

Gradient descent
Gradient descent with momentum
Gradient descent with Nesterov momentum
AdaGrad
Adam

 

and check what mean-squared-error you get after the notebook completes its optimizer iterations.

 

(3) Replace alpha +_beta*x + gamma*x**2 with alpha +_beta*x + 1e-6*gamma*x**2 and simultaneously make the value of gamma_true million times larger than previously. Then check again the performance of 

 

Gradient descent
Gradient descent with momentum
Gradient descent with Nesterov momentum
AdaGrad
Adam

 

as in step 2.

 

(4) Write a short summary of your results (ideally in the PDF format). Do they correspond to what you expected based on your intuition about the optimizers?

 

In any of the above please feel free to change the number of data points from 30 to any larger number.

 

It would be great if you could then combine all of your notebooks and the PDF into one zip file and send it to me at fabinger@gmail.com. At this moment, we don't have a fixed deadline, but the earlier you can do this, the earlier I'll be able to get back to you.

 

Best,
Michal
